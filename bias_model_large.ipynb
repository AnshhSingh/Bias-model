{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets evaluate wandb scikit-learn\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0XiYM8Yr0Q0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5LEJ9GsV7254"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import wandb\n",
        "import evaluate\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9urIWZny0WsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf drive/MyDrive/data.tar.gz"
      ],
      "metadata": {
        "id": "HzEnrv830baz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "JSON_DIR = \"./data/jsons\"\n",
        "SPLIT_TYPES = ['random']\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "MAX_LENGTH = 512"
      ],
      "metadata": {
        "id": "Kepm4A0-0hsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_data = {}\n",
        "missing_count = 0\n",
        "\n",
        "for filename in os.listdir(JSON_DIR):\n",
        "    if filename.endswith(\".json\"):\n",
        "        try:\n",
        "            with open(os.path.join(JSON_DIR, filename), 'r') as f:\n",
        "                data = json.load(f)\n",
        "                if 'ID' in data and 'content_original' in data and 'bias' in data:\n",
        "                    id_to_data[data['ID']] = data\n",
        "                else:\n",
        "                    print(f\"Skipping invalid JSON: {filename}\")\n",
        "                    continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "            missing_count += 1\n",
        "\n",
        "print(f\"Successfully loaded {len(id_to_data)} articles\")\n",
        "if missing_count > 0:\n",
        "    print(f\"Warning: Failed to load {missing_count} files\")"
      ],
      "metadata": {
        "id": "WOtWhCSY1ATU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_split(split_type, split_name):\n",
        "    \"\"\"Load TSV files from ./data/splits/[split_type]/[split_name].tsv\"\"\"\n",
        "    tsv_path = f\"./data/splits/{split_type}/{split_name}.tsv\"\n",
        "    print(f\"ðŸ”„ Looking for split file at: {tsv_path}\")  # Debug path\n",
        "\n",
        "    if not os.path.exists(tsv_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\" Missing split file! Verify these exist:\\n\"\n",
        "            f\"1. Directory structure: ./data/splits/{split_type}/\\n\"\n",
        "            f\"2. File name: {split_name}.tsv\\n\"\n",
        "            f\"3. File extension: .tsv (not .txt)\"\n",
        "        )\n",
        "\n",
        "    ids, labels = [], []\n",
        "    with open(tsv_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if len(row) == 2:\n",
        "                ids.append(row[0])\n",
        "                labels.append(int(row[1]))\n",
        "\n",
        "    print(f\"âœ… Loaded {len(ids)} samples from {tsv_path}\")\n",
        "    return ids, labels"
      ],
      "metadata": {
        "id": "PA8WYTa81KTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiasDataset(Dataset):\n",
        "    def __init__(self, ids, labels, id_to_data, tokenizer, max_length):\n",
        "        self.ids = ids\n",
        "        self.labels = labels\n",
        "        self.id_to_data = id_to_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.msax_length = max_length\n",
        "\n",
        "        # Verify all IDs exist\n",
        "        missing_ids = [id_ for id_ in ids if id_ not in id_to_data]\n",
        "        if missing_ids:\n",
        "            print(f\"Warning: {len(missing_ids)} IDs not found in data\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            article = self.id_to_data[self.ids[idx]]\n",
        "            encoding = self.tokenizer(\n",
        "                article['content_original'],\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "            }\n",
        "        except KeyError:\n",
        "            print(f\"Missing article for ID: {self.ids[idx]}\")\n",
        "            return None  # Will be handled by Trainer"
      ],
      "metadata": {
        "id": "Kg28xMZi1K-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    try:\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "            \"f1_weighted\": f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
        "            \"f1_macro\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing metrics: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "udUx3Sxn1N9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WandB\n",
        "try:\n",
        "    wandb.login()\n",
        "except Exception as e:\n",
        "    print(f\"WandB login failed: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "QzUKXqzD1VVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split_type in SPLIT_TYPES:\n",
        "    print(f\"\\n{'='*40}\\nTraining on {split_type} split\\n{'='*40}\")\n",
        "\n",
        "    try:\n",
        "        # Load splits\n",
        "\n",
        "        train_ids, train_labels = load_split(split_type, 'train')\n",
        "        val_ids, val_labels = load_split(split_type, 'valid')\n",
        "        test_ids, test_labels = load_split(split_type, 'test')\n",
        "\n",
        "        # Initialize model components\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = RobertaForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=3,\n",
        "            id2label={0: \"left\", 1: \"center\", 2: \"right\"}\n",
        "        ).to('cuda')\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = BiasDataset(train_ids, train_labels, id_to_data, tokenizer, MAX_LENGTH)\n",
        "        val_dataset = BiasDataset(val_ids, val_labels, id_to_data, tokenizer, MAX_LENGTH)\n",
        "        test_dataset = BiasDataset(test_ids, test_labels, id_to_data, tokenizer, MAX_LENGTH)\n",
        "\n",
        "        # Training setup\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=os.path.join(\"results/{split_type}\"),\n",
        "            evaluation_strategy='epoch',\n",
        "            save_strategy='epoch',\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=4,\n",
        "            weight_decay=0.01,\n",
        "            fp16=True,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1_macro',\n",
        "            report_to=\"wandb\",\n",
        "            logging_steps=50,\n",
        "            push_to_hub=False,\n",
        "            warmup_steps=100,                          # Gradually ramp up LR(helps a bit with accuracy later)\n",
        "            lr_scheduler_type=\"linear\",\n",
        "        )\n",
        "\n",
        "        # Init trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        # WandB\n",
        "        wandb.init(\n",
        "            project=\"political-bias-detection\",\n",
        "            name=f\"{MODEL_NAME}-{split_type}\",\n",
        "            config=training_args.to_dict()\n",
        "        )\n",
        "\n",
        "        # Training\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "\n",
        "        # Final evaluation\n",
        "        test_results = trainer.evaluate(test_dataset)\n",
        "        print(f\"\\nTest results ({split_type}):\")\n",
        "        print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "        print(f\"Weighted F1: {test_results['eval_f1_weighted']:.4f}\")\n",
        "        print(f\"Macro F1: {test_results['eval_f1_macro']:.4f}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during {split_type} training: {e}\")\n",
        "        raise\n",
        "\n",
        "        # save_path = f\"./saved_models/{split_type}_model\"\n",
        "        # os.makedirs(save_path, exist_ok=True)\n",
        "        # model.save_pretrained(save_path)\n",
        "        # tokenizer.save_pretrained(save_path)\n",
        "        # print(f\"\\n Model saved to: {save_path}\")\n",
        "        # print(f\"Contents: {os.listdir(save_path)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TjaNYxyo1vSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -cvf large.tar.gz results/random/checkpoint-13992"
      ],
      "metadata": {
        "id": "SmVOy51EXmrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp large.tar.gz /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "gywpLCyBe0oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "save_dir = f\"/content/drive/MyDrive/saved_models/{split_type}_model\"\n",
        "\n",
        "\n",
        "import os\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(f\"Model saved to: {save_dir}\")\n",
        "\n",
        "import wandb\n",
        "api = wandb.Api()\n",
        "\n",
        "sweep = api.sweep(\"\")\n",
        "runs = sorted(sweep.runs,\n",
        "  key=lambda run: run.summary.get(\"val_acc\", 0), reverse=True)\n",
        "val_acc = runs[0].summary.get(\"val_acc\", 0)\n",
        "print(f\"Best run {runs[0].name} with {val_acc}% validation accuracy\")\n",
        "\n",
        "runs[0].file(\"model.h5\").download(replace=True)\n",
        "print(\"Best model saved to model-best.h5\")"
      ],
      "metadata": {
        "id": "NNpxyy6sURO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}